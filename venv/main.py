# main.py - ìƒì„¸ ë””ë²„ê¹… ë° í˜ë¥´ì†Œë‚˜ ê°•í™” ë²„ì „ (v6.1)
# ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ì½”ë“œ ë‚´ ëª¨ë“  í˜ë¥´ì†Œë‚˜ ê´€ë ¨ í…ìŠ¤íŠ¸ë¥¼ ì™„ì „íˆ ì œê±°
# ì±—ë´‡ì˜ í˜ë¥´ì†Œë‚˜ëŠ” ë°˜ë“œì‹œ jia_persona.txtì— ì‘ì„±í•´ì•¼í•©ë‹ˆë‹¤.

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import uvicorn
from typing import Optional, AsyncGenerator, Dict, Any
from datetime import datetime
import os
import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from contextlib import asynccontextmanager
import psutil
from PIL import Image
import base64
import io
import traceback
import json
import asyncio
import sys
import random
import re

# ë””ë²„ê¹… ì‹œìŠ¤í…œ ì„í¬íŠ¸
from debug_logger import init_debugger, get_debugger, debug_function, monitor_system_resources

# ê¸°ì¡´ ì„í¬íŠ¸ë“¤
try:
    from api_keys import GOOGLE_API_KEY, OPENROUTER_API_KEY
    from services.intent_classifier import classify_intent
    from services.story_generator import generate_enhanced_story
    from services.code_generator import generate_enhanced_code
    from services.chat_handlers import reset_memory as reset_memory_handler
    from utils.memory_builder import MemoryBuilder
    from utils.summarizer import MemorySummarizer
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}", file=sys.stderr)
    print("ğŸ’¡ services, utils í´ë”ì™€ api_keys.py íŒŒì¼ì´ main.pyì™€ ê°™ì€ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.", file=sys.stderr)
    sys.exit(1)

# =========================
# ì„¤ì • ë¡œë“œ (config.json)
# =========================
def load_config(filename: str = "config.json") -> Dict[str, Any]:
    """config.jsonì—ì„œ ì„¤ì •ì„ ì½ì–´ì˜µë‹ˆë‹¤. ì—†ìœ¼ë©´ ìµœì†Œ ê¸°ë³¸ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    defaults = {
        "local_model_path": r"F:\venv\MiniCPM-V",
        "log_level": "DEBUG"
    }
    try:
        if os.path.exists(filename):
            with open(filename, "r", encoding="utf-8") as f:
                cfg = json.load(f)
            # ê¸°ë³¸ê°’ ë³‘í•©
            for k, v in defaults.items():
                if k not in cfg:
                    cfg[k] = v
            return cfg
        else:
            return defaults
    except Exception:
        return defaults


CONFIG = load_config()

# ë””ë²„ê¹… ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´ ë° config)
DEBUG_ENABLED = os.getenv("JIA_DEBUG", "true").lower() == "true"
LOG_TO_FILE = os.getenv("JIA_LOG_FILE", "true").lower() == "true"

# ë””ë²„ê±° ì´ˆê¸°í™”
debugger = init_debugger(DEBUG_ENABLED, LOG_TO_FILE)

# ì „ì—­ ë³€ìˆ˜ë“¤
minicpm_model = None
minicpm_tokenizer = None
JIA_CORE_PERSONA = ""  # ì‹¤ì œ í˜ë¥´ì†Œë‚˜ëŠ” jia_persona.txtì—ì„œë§Œ ê´€ë¦¬
memory_builder = None
summarizer = None


def create_default_persona_file(filename="jia_persona.txt"):
    """ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ íŒŒì¼ ìƒì„± (ë¹ˆ íŒŒì¼) â€” ì½”ë“œ ë‚´ í˜ë¥´ì†Œë‚˜ í…ìŠ¤íŠ¸ëŠ” ì—†ìŒ"""
    debugger.debug(f"í˜ë¥´ì†Œë‚˜ íŒŒì¼ ìƒì„±(í™•ì¸): {filename}", "PERSONA")
    if not os.path.exists(filename):
        with open(filename, "w", encoding="utf-8") as f:
            # ì˜ë„ì ìœ¼ë¡œ ë¹ˆ íŒŒì¼ ìƒì„± â€” ì‚¬ìš©ìê°€ ì§ì ‘ ë‚´ìš©ì„ ì‘ì„±í•´ì•¼ í•¨
            f.write("")
        debugger.success(f"ë¹ˆ {filename} íŒŒì¼ ìƒì„± ì™„ë£Œ. í˜ë¥´ì†Œë‚˜ë¥¼ ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”.", "PERSONA")


def load_jia_persona(filename="jia_persona.txt"):
    """í˜ë¥´ì†Œë‚˜ íŒŒì¼ ë¡œë“œ (íŒŒì¼ì´ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ â€” í˜ë¥´ì†Œë‚˜ëŠ” ì™¸ë¶€ë¡œë§Œ ê´€ë¦¬)"""
    try:
        debugger.debug(f"í˜ë¥´ì†Œë‚˜ ë¡œë“œ ì‹œë„: {filename}", "PERSONA")
        with open(filename, "r", encoding="utf-8") as f:
            persona = f.read().strip()
        if not persona:
            debugger.warning(f"{filename} íŒŒì¼ì˜ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í˜ë¥´ì†Œë‚˜ëŠ” ì™¸ë¶€ íŒŒì¼ì— ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.", "PERSONA")
            return ""  # ì½”ë“œ ë‚´ í˜ë¥´ì†Œë‚˜ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ
        debugger.success(f"í˜ë¥´ì†Œë‚˜ ë¡œë“œ ì™„ë£Œ: {len(persona)}ì", "PERSONA")
        return persona
    except FileNotFoundError:
        debugger.warning(f"{filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¹ˆ íŒŒì¼ ìƒì„±", "PERSONA")
        create_default_persona_file(filename)
        return load_jia_persona(filename)  # ìƒì„± í›„ ë‹¤ì‹œ ë¡œë“œ
    except Exception as e:
        debugger.error(f"í˜ë¥´ì†Œë‚˜ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}", "PERSONA")
        return ""


@debug_function
def load_minicpm_model():
    """MiniCPM-V ëª¨ë¸ ë¡œë“œ"""
    global minicpm_model, minicpm_tokenizer

    debugger.info("MiniCPM-V ëª¨ë¸ ë¡œë”© ì‹œì‘", "MODEL")
    debugger.debug("=" * 60, "MODEL")

    # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
    try:
        available_memory_gb = psutil.virtual_memory().available / (1024 ** 3)
        debugger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {available_memory_gb:.2f} GB", "SYSTEM")

        if torch.cuda.is_available():
            try:
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
                debugger.info(f"GPU: {gpu_name}", "GPU")
                debugger.info(f"GPU ë©”ëª¨ë¦¬: {gpu_memory_gb:.2f} GB", "GPU")
            except Exception as e:
                debugger.warning(f"GPU ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", "GPU")

            torch.cuda.empty_cache()
            debugger.success("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ", "GPU")
        else:
            debugger.warning("CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰", "GPU")

    except Exception as e:
        debugger.error(f"ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}", "SYSTEM")

    try:
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì • (configì—ì„œ ë¡œë“œ)
        local_model_path = CONFIG.get("local_model_path", r"F:\venv\MiniCPM-V")
        debugger.debug(f"ëª¨ë¸ ê²½ë¡œ í™•ì¸: {local_model_path}", "MODEL")

        if not os.path.exists(local_model_path):
            debugger.error(f"ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {local_model_path}", "MODEL")
            return False

        debugger.success(f"ëª¨ë¸ ê²½ë¡œ í™•ì¸ ì™„ë£Œ", "MODEL")

        # ì–‘ìí™” ì„¤ì •
        debugger.debug("ì–‘ìí™” ì„¤ì • ì¤€ë¹„ ì¤‘...", "MODEL")
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        debugger.success("ì–‘ìí™” ì„¤ì • ì™„ë£Œ", "MODEL")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        debugger.info("í† í¬ë‚˜ì´ì € ë¡œë”© ì‹œì‘...", "MODEL")
        start_time = time.time()

        tokenizer = AutoTokenizer.from_pretrained(
            local_model_path,
            trust_remote_code=True
        )

        load_time = time.time() - start_time
        debugger.success(f"í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ ({load_time:.2f}ì´ˆ)", "MODEL")

        # ëª¨ë¸ ë¡œë“œ
        debugger.info("MiniCPM-V ëª¨ë¸ ë¡œë”© ì‹œì‘...", "MODEL")
        start_time = time.time()

        model = AutoModelForCausalLM.from_pretrained(
            local_model_path,
            trust_remote_code=True,
            quantization_config=quantization_config,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            low_cpu_mem_usage=True,
        )

        load_time = time.time() - start_time
        debugger.success(f"MiniCPM-V ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({load_time:.2f}ì´ˆ)", "MODEL")

        # íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            debugger.success("íŒ¨ë”© í† í° ì„¤ì • ì™„ë£Œ", "MODEL")

        minicpm_model = model
        minicpm_tokenizer = tokenizer

        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        debugger.info(f"ëª¨ë¸ ë””ë°”ì´ìŠ¤: {model.device}", "MODEL")
        debugger.info(f"ëª¨ë¸ dtype: {model.dtype}", "MODEL")
        debugger.success("MiniCPM-V ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ!", "MODEL")

        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if torch.cuda.is_available():
            try:
                allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)
                reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)
                debugger.info(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©: {allocated:.2f} GB (ì˜ˆì•½: {reserved:.2f} GB)", "GPU")
            except Exception as e:
                debugger.warning(f"GPU ë©”ëª¨ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", "GPU")

        return True

    except torch.cuda.OutOfMemoryError as e:
        debugger.critical(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}", "MODEL")
        torch.cuda.empty_cache()
        return False

    except Exception as e:
        debugger.critical(f"MiniCPM-V ëª¨ë¸ ë¡œë”© ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", "MODEL")
        debugger.debug(f"ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}", "MODEL")
        return False


@debug_function
async def chat_with_minicpm_text_only(user_message: str, context_info: Dict = None) -> str:
    """ê°œì„ ëœ MiniCPM-V í…ìŠ¤íŠ¸ ì±„íŒ… (í˜ë¥´ì†Œë‚˜ëŠ” jia_persona.txtì—ì„œë§Œ ì œê³µë˜ì–´ì•¼ í•¨)"""

    if minicpm_model is None or minicpm_tokenizer is None:
        debugger.error("MiniCPM-V ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤", "CHAT")
        return "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ ì£¼ì„¸ìš”."

    start_time = time.time()
    debugger.chat_debug(user_message, "", 0, function="chat_with_minicpm_text_only", line=0)

    try:
        # 1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        debugger.model_debug("PROMPT_BUILD", "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë° ë©”ì‹œì§€ êµ¬ì„± ì¤‘...")

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í˜ë¥´ì†Œë‚˜) ë¡œë“œ â€” ì½”ë“œ ë‚´ì—ëŠ” í˜ë¥´ì†Œë‚˜ í…ìŠ¤íŠ¸ê°€ ì—†ìŒ.
        system_prompt = JIA_CORE_PERSONA or ""
        debugger.model_debug("SYSTEM_PROMPT", f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ: {len(system_prompt)}ì")

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ë° í˜„ì¬ ë©”ì‹œì§€ êµ¬ì„±
        conversation_history = ""
        if context_info and context_info.get("context_summary"):
            conversation_history += f"[ì´ì „ ëŒ€í™” ìš”ì•½]:\n{context_info['context_summary']}\n\n"
            debugger.model_debug("CONTEXT", f"ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€: {len(context_info['context_summary'])}ì")

        conversation_history += f"ì‚¬ìš©ì: {user_message}"

        # í”„ë¡¬í”„íŠ¸ êµ¬ì¡°: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸(í˜ë¥´ì†Œë‚˜)ê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ êµ¬ì„±
        # ì½”ë“œ ë‚´ì— í˜ë¥´ì†Œë‚˜ ê·œì¹™ì„ í•˜ë“œì½”ë”©í•˜ì§€ ì•ŠìŒ â€” í˜ë¥´ì†Œë‚˜ëŠ” jia_persona.txtë¡œë§Œ ê´€ë¦¬ë©ë‹ˆë‹¤.
        if system_prompt:
            full_user_prompt = f"""{system_prompt}

---
ì•„ë˜ ëŒ€í™”ì— ì´ì–´ì„œ, ìœ„ì— ì œê³µëœ í˜ë¥´ì†Œë‚˜ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ì—¬ ëŒ€ë‹µí•˜ì‹œì˜¤.

[ëŒ€í™” ë‚´ìš©]
{conversation_history}

[ìµœì¢… ì§€ì‹œ]
ì±—ë´‡:"""
        else:
            # í˜ë¥´ì†Œë‚˜ê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” ì¼ë°˜ì ì¸ ëŒ€í™” í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©
            full_user_prompt = f"""{conversation_history}

ì±—ë´‡:"""

        messages = [
            {"role": "user", "content": full_user_prompt}
        ]
        debugger.model_debug("MESSAGES", f"ë©”ì‹œì§€ êµ¬ì„± ì™„ë£Œ: 1ê°œ (ìµœì¢… ê°•í™” í”„ë¡¬í”„íŠ¸)")

        # 2ë‹¨ê³„: ìƒì„± ì„¤ì • (ì•ˆì •ì„± ìœ„ì£¼ë¡œ ì¡°ì •)
        generation_config = {
            "temperature": 0.7,
            "top_p": 0.8,
            "repetition_penalty": 1.2,
        }
        debugger.model_debug("CONFIG", f"ìƒì„± ì„¤ì •: temp={generation_config['temperature']}, penalty={generation_config['repetition_penalty']}")

        # 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ìƒì„± (model.chat ì‚¬ìš©)
        debugger.model_debug("GENERATE", "model.chatìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘...")
        generation_start = time.time()

        with torch.no_grad():
            response = minicpm_model.chat(
                image=None,
                msgs=messages,
                tokenizer=minicpm_tokenizer,
                **generation_config
            )

        generation_time = time.time() - generation_start
        debugger.success(f"í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ ({generation_time:.2f}ì´ˆ)", "MODEL")
        debugger.debug(f"ì›ë³¸ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {repr(response[:140])}", "MODEL")

        # 4ë‹¨ê³„: ì‘ë‹µ í›„ì²˜ë¦¬
        debugger.model_debug("CLEANUP", "ì‘ë‹µ í›„ì²˜ë¦¬ ì¤‘...")
        cleaned_response = clean_response(response, user_message)

        total_time = time.time() - start_time

        debugger.success(f"ì±„íŒ… ì²˜ë¦¬ ì™„ë£Œ (ì´ {total_time:.2f}ì´ˆ)", "CHAT")
        debugger.chat_debug(user_message, cleaned_response, total_time)

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return cleaned_response

    except torch.cuda.OutOfMemoryError as cuda_error:
        debugger.critical(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì±„íŒ… ì‹¤íŒ¨: {cuda_error}", "CHAT")
        torch.cuda.empty_cache()
        return "ì ê¹, GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•´ì„œ ì²˜ë¦¬ê°€ ì§€ì—°ë˜ê³  ìˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    except Exception as e:
        total_time = time.time() - start_time
        debugger.critical(f"MiniCPM-V í…ìŠ¤íŠ¸ ì±„íŒ… ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ({total_time:.2f}ì´ˆ): {e}", "CHAT")
        debugger.debug(f"ì˜¤ë¥˜ ë°œìƒ ìœ„ì¹˜ ì¶”ì : {traceback.format_exc()}", "CHAT")
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”. [ì˜¤ë¥˜ì½”ë“œ: {type(e).__name__}]"


@debug_function
def clean_response(response: str, user_message: str = "") -> str:
    """ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ê²€ì¦ (ë””ë²„ê¹… í¬í•¨)"""

    debugger.debug(f"ì‘ë‹µ ì •ë¦¬ ì‹œì‘: ì›ë³¸ {len(response)}ì", "CLEANUP")

    if not response:
        debugger.warning("ë¹ˆ ì‘ë‹µ ê°ì§€", "CLEANUP")
        return "ì–´... ë­”ê°€ ë§í•˜ë ¤ê³  í–ˆëŠ”ë° ìƒê°ì´ ì•ˆ ë‚˜ë„¤! ë‹¤ì‹œ ë§í•´ì¤„ë˜?"

    original_response = response

    # í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ ë§ˆì§€ë§‰ 'ì±—ë´‡:' ì´í›„ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ
    if 'ì±—ë´‡:' in response:
        response = response.split('ì±—ë´‡:')[-1].strip()

    # 'ì´ë¦„:'ê³¼ ê°™ì€ ë°œí™”ì í‘œì‹œ ì œê±° (ë„ˆë¬´ ì§§ì€ ì´ë¦„ì´ë©´ ì œê±°)
    if ":" in response:
        parts = response.split(":", 1)
        if len(parts[0]) < 10:
            response = parts[1].strip()
            debugger.debug(f"ì‘ë‹µ ì‹œì‘ '{parts[0]}:' ì œê±°", "CLEANUP")

    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ë° í† í° ì œê±°
    cleanup_patterns = [
        "<|system|>", "<|user|>", "<|assistant|>", "<|endoftext|>",
        "[ê¸°ì–µ]", "[ì‹œìŠ¤í…œ]", "System:", "User:", "Assistant:",
        "<s>", "</s>", "<pad>", "[PAD]", "ì‚¬ìš©ì:"
    ]

    for pattern in cleanup_patterns:
        if pattern in response:
            response = response.replace(pattern, "")
            debugger.debug(f"ì œê±°ëœ íŒ¨í„´: {pattern}", "CLEANUP")

    # ì¤„ë°”ê¿ˆ ì •ë¦¬
    response = response.replace("\r\n", "\n")
    while "\n\n\n" in response:
        response = response.replace("\n\n\n", "\n\n")
    response = response.strip()

    # ì‚¬ìš©ì ë©”ì‹œì§€ ë°˜ë³µ ì œê±°
    if user_message and user_message in response:
        response = response.replace(user_message, "").strip()
        debugger.debug("ì‚¬ìš©ì ë©”ì‹œì§€ ë°˜ë³µ ì œê±°", "CLEANUP")

    # ì˜ë¯¸ì—†ëŠ” ì‘ë‹µ í•„í„°ë§
    if len(response.strip()) < 2:
        debugger.warning(f"ë„ˆë¬´ ì§§ì€ ì‘ë‹µ: '{response}'", "CLEANUP")
        return "ì–´... ë­”ê°€ ë§í•˜ë ¤ê³  í–ˆëŠ”ë° ìƒê°ì´ ì•ˆ ë‚˜ë„¤! ë‹¤ì‹œ ë§í•´ë³¼ë˜?"

    # ë°˜ë³µ íŒ¨í„´ ì œê±° (ì—°ì† ë™ì¼ ë‹¨ì–´ 3íšŒ ì´ìƒ)
    words = response.split()
    if len(words) > 4:
        cleaned_words = [words[0]]
        for i in range(1, len(words)):
            if not (i >= 2 and words[i] == words[i - 1] == words[i - 2]):
                cleaned_words.append(words[i])

        if len(cleaned_words) < len(words):
            response = " ".join(cleaned_words)
            debugger.debug("ë°˜ë³µ íŒ¨í„´ ì œê±°", "CLEANUP")

    final_length = len(response)
    debugger.success(f"ì‘ë‹µ ì •ë¦¬ ì™„ë£Œ: {len(original_response)}ì â†’ {final_length}ì", "CLEANUP")

    return response.strip()


# íƒ€ì´í•‘ íš¨ê³¼ë¥¼ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (ê°œì„ ëœ ë²„ì „)
async def generate_typing_response(text: str):
    """íƒ€ì´í•‘ íš¨ê³¼ë¥¼ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ê°œì„ )
    - ë¬¸ì¥ ë¶€í˜¸ ë° ë¬¸ì ê¸°ë°˜ ì§€ì—° ì ìš©
    - ëˆ„ì ëœ í…ìŠ¤íŠ¸(ì§„í–‰ì¤‘ ë‚´ìš©)ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì „ì†¡
    - ì‘ì€ ëœë¤ ì§€í„° ì¶”ê°€ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ê° ì œê³µ
    - SSE í˜•ì‹('data: {...}\n\n')ìœ¼ë¡œ ì „ì†¡
    """
    debugger.debug(f"íƒ€ì´í•‘ íš¨ê³¼ ì‹œì‘: {len(text)}ì", "STREAMING")

    if not text or len(text.strip()) == 0:
        text = "ìŒ... ë¬´ìŠ¨ ë§ì„ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ë„¤. ë‹¤ì‹œ í•œë²ˆ ë¬¼ì–´ë´ ì¤„ë˜?"
        debugger.warning("ìŠ¤íŠ¸ë¦¬ë°í•  ë‚´ìš©ì´ ì—†ì–´ ê¸°ë³¸ ë©”ì‹œì§€ë¡œ ëŒ€ì²´", "STREAMING")

    # ì¤„ì„/ì •ë¦¬
    text = text.replace("\r\n", "\n").strip()

    # ì¦‰ì‹œ ì²« ì‘ë‹µ(ë¹„ì–´ìˆëŠ” í”Œë ˆì´ìŠ¤í™€ë”) â€” í´ë¼ì´ì–¸íŠ¸ì— ë°”ë¡œ ì—°ê²°ë˜ì—ˆìŒì„ ì•Œë¦¼
    initial_payload = {"content": "", "finished": False}
    yield f"data: {json.dumps(initial_payload, ensure_ascii=False)}\n\n"

    current_text = ""
    since_last_yield = 0

    # íŒë³„ìš© ë¬¸ì¥ë¶€í˜¸ ì§‘í•©
    sentence_punct = set(['.', '?', '!', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦'])
    newline_chars = set(['\n'])

    for idx, ch in enumerate(text):
        current_text += ch
        since_last_yield += 1

        # ë¬¸ì ìœ í˜•ì— ë”°ë¥¸ ê¸°ë³¸ ì§€ì—° (í•œ ë¬¸ìë‹¹)
        # í•œê¸€ / CJK ë“± ë¹„-ASCII ë¬¸ìì— ëŒ€í•´ ë” ë¹ ë¥´ê²Œ, ASCIIëŠ” ì•½ê°„ ëŠë¦¬ê²Œ
        try:
            is_ascii = ord(ch) < 128
        except Exception:
            is_ascii = False

        base_delay = 0.025 if is_ascii else 0.015  # ì¡°ì ˆê°’
        jitter = random.uniform(-0.006, 0.01)
        per_char_delay = max(0.001, base_delay + jitter)

        # ì „ì†¡ ì¡°ê±´:
        # 1) ë¬¸ì¥ë¶€í˜¸ê°€ ë‚˜ì˜¤ë©´ (ì¦‰ì‹œ ì „ì†¡ â€” ë¬¸ì¥ ë‹¨ìœ„)
        # 2) ê°œí–‰ ë¬¸ìê°€ ë‚˜ì˜¤ë©´
        # 3) ì¼ì • ë¬¸ì(í† í°) ìˆ˜ê°€ ìŒ“ì˜€ì„ ë•Œ (ì§„í–‰ í‘œì‹œìš©)
        threshold = 6 if not is_ascii else 10  # í•œê¸€ì€ ë” ìì£¼ ë‚´ë³´ë‚´ê¸°
        if ch in sentence_punct or ch in newline_chars or since_last_yield >= threshold or idx == len(text) - 1:
            finished = (idx == len(text) - 1)
            payload = {"content": current_text, "finished": finished}
            yield f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"
            since_last_yield = 0

            # ì¶”ê°€ì  ë¬¸ì¥ë¶€í˜¸ í›„ íœ´ì§€ (ë¬¸ì¥ ê²½ê³„ì—ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ì •ì§€)
            if ch in sentence_punct:
                await asyncio.sleep(0.08 + random.uniform(0.0, 0.06))
            elif ch in newline_chars:
                await asyncio.sleep(0.06 + random.uniform(0.0, 0.06))

        # ë¬¸ìë³„ ê¸°ë³¸ ì§€ì—°
        await asyncio.sleep(per_char_delay)

    debugger.success("íƒ€ì´í•‘ íš¨ê³¼ ì™„ë£Œ", "STREAMING")


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    global JIA_CORE_PERSONA, memory_builder, summarizer

    debugger.info("ì±—ë´‡ ì‹œìŠ¤í…œ ì‹œì‘", "SYSTEM")
    debugger.debug("=" * 60, "SYSTEM")

    # í˜ë¥´ì†Œë‚˜ ì´ˆê¸°í™” (íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ íŒŒì¼ ìƒì„±)
    create_default_persona_file()
    JIA_CORE_PERSONA = load_jia_persona()
    debugger.success(f"í˜ë¥´ì†Œë‚˜ ë¡œë“œ ì™„ë£Œ: {len(JIA_CORE_PERSONA)}ì", "PERSONA")

    # ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    debugger.info("ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...", "MEMORY")
    try:
        memory_builder = MemoryBuilder()
        summarizer = MemorySummarizer()

        if not summarizer.load_personality_profile():
            debugger.info("ìƒˆë¡œìš´ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ì¤‘...", "MEMORY")
            summarizer.create_personality_profile("ë‹´")
        else:
            debugger.success("ê¸°ì¡´ ì‚¬ìš©ì í”„ë¡œí•„ ë¡œë“œ ì™„ë£Œ", "MEMORY")

        debugger.success("ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!", "MEMORY")

    except Exception as e:
        debugger.error(f"ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", "MEMORY")

    # ëª¨ë¸ ë¡œë“œ
    if load_minicpm_model():
        debugger.success("ëª¨ë“  ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!", "SYSTEM")
        monitor_system_resources()
    else:
        debugger.warning("ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì œí•œëœ ê¸°ëŠ¥ìœ¼ë¡œ ì‹¤í–‰", "SYSTEM")

    debugger.debug("=" * 60, "SYSTEM")

    yield

    debugger.info("ì±—ë´‡ ì‹œìŠ¤í…œ ì¢…ë£Œ", "SYSTEM")


# FastAPI ì•± ì„¤ì •
app = FastAPI(
    title="ì±—ë´‡ (ë””ë²„ê·¸ ë²„ì „ v6.1)",
    description="ìƒì„¸ ë””ë²„ê¹… ë° í˜ë¥´ì†Œë‚˜ ì™¸ë¶€í™”ëœ MiniCPM-V ê¸°ë°˜ AI ì±—ë´‡",
    version="6.1.0",
    lifespan=lifespan
)


# ìš”ì²­ ëª¨ë¸ë“¤
class ChatRequest(BaseModel):
    message: str


class ImageChatRequest(BaseModel):
    message: str
    image_data: str


@app.get("/")
async def read_root():
    debugger.api_debug("/", 200, "ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ")
    return {
        "message": "ğŸ¤– ì±—ë´‡ (ë””ë²„ê·¸ ë²„ì „ v6.1)",
        "version": "6.1.0",
        "debug_enabled": DEBUG_ENABLED,
        "log_to_file": LOG_TO_FILE
    }


@app.post("/chat")
async def chat(request: ChatRequest):
    """ê°œì„ ëœ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (ìƒì„¸ ë””ë²„ê¹…)"""
    user_message = request.message
    start_time = time.time()

    debugger.info(f"ì±„íŒ… ìš”ì²­ ìˆ˜ì‹ : {user_message[:50]}...", "API")

    try:
        # ì˜ë„ ë¶„ë¥˜
        debugger.debug("ì˜ë„ ë¶„ë¥˜ ì‹œì‘", "INTENT")
        intent = classify_intent(user_message)
        debugger.success(f"ì˜ë„ ë¶„ë¥˜ ê²°ê³¼: {intent}", "INTENT")

        if intent == "creative_writing":
            debugger.info("ìŠ¤í† ë¦¬ ìƒì„± ëª¨ë“œ ì‹¤í–‰", "STORY")
            story_type = "short_story"
            if "ê¸´" in user_message or "ì¥í¸" in user_message:
                story_type = "long_story"
            elif "ì¤‘í¸" in user_message:
                story_type = "medium_story"

            debugger.debug(f"ìŠ¤í† ë¦¬ íƒ€ì…: {story_type}", "STORY")

            response_text = await generate_enhanced_story(
                user_message, story_type, JIA_CORE_PERSONA, GOOGLE_API_KEY
            )
            api_tag = "[Gemini API - ìŠ¤í† ë¦¬]"

            try:
                memory_builder.save_dialogue(user_message, response_text)
                debugger.success("ìŠ¤í† ë¦¬ ëŒ€í™” ë©”ëª¨ë¦¬ ì €ì¥ ì™„ë£Œ", "MEMORY")
            except Exception as mem_error:
                debugger.error(f"ìŠ¤í† ë¦¬ ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {mem_error}", "MEMORY")

        elif intent == "code_generation":
            debugger.info("ì½”ë“œ ìƒì„± ëª¨ë“œ ì‹¤í–‰", "CODE")

            code_result = await generate_enhanced_code(
                user_message, JIA_CORE_PERSONA, OPENROUTER_API_KEY
            )

            if code_result.get("error"):
                response_text = code_result["error"]
                debugger.warning(f"ì½”ë“œ ìƒì„± ì˜¤ë¥˜: {response_text}", "CODE")
            else:
                response_text = f"## ğŸ’» {code_result['title']}\n\n{code_result['description']}\n\n### HTML\n```html\n{code_result['html']}\n```\n\n### CSS\n```css\n{code_result['css']}\n```\n\n### JavaScript\n```javascript\n{code_result['js']}\n```"
                debugger.success("ì½”ë“œ ìƒì„± ì™„ë£Œ", "CODE")

            api_tag = "[OpenRouter API - ì½”ë“œ]"

            try:
                memory_builder.save_dialogue(user_message, response_text)
                debugger.success("ì½”ë“œ ëŒ€í™” ë©”ëª¨ë¦¬ ì €ì¥ ì™„ë£Œ", "MEMORY")
            except Exception as mem_error:
                debugger.error(f"ì½”ë“œ ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {mem_error}", "MEMORY")

        else:
            debugger.info("ì¼ë°˜ ì±„íŒ… ëª¨ë“œ - MiniCPM-V ì‹¤í–‰", "CHAT")

            # ë©”ëª¨ë¦¬ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            try:
                context = memory_builder.build_context_from_query(user_message)
                debugger.success(f"ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ: {bool(context)}", "MEMORY")
            except Exception as ctx_error:
                debugger.error(f"ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {ctx_error}", "MEMORY")
                context = {}

            # MiniCPM-Vë¡œ ì‘ë‹µ ìƒì„±
            response_text = await chat_with_minicpm_text_only(user_message, context)
            api_tag = "[MiniCPM-V - í†µí•© ëª¨ë¸]"

            # ë©”ëª¨ë¦¬ì— ì €ì¥
            try:
                memory_builder.save_dialogue(user_message, response_text)
                debugger.success("ì±„íŒ… ëŒ€í™” ë©”ëª¨ë¦¬ ì €ì¥ ì™„ë£Œ", "MEMORY")
            except Exception as mem_error:
                debugger.error(f"ì±„íŒ… ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {mem_error}", "MEMORY")

        total_time = time.time() - start_time
        final_response = f"{api_tag}\n\n{response_text}"

        debugger.success(f"ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ ({total_time:.2f}ì´ˆ): {len(response_text)}ì", "API")
        debugger.api_debug("/chat", 200, f"ì²˜ë¦¬ ì™„ë£Œ ({total_time:.2f}ì´ˆ)")

        return {"response": final_response}

    except Exception as e:
        total_time = time.time() - start_time
        error_msg = f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜ ({total_time:.2f}ì´ˆ): {str(e)}"

        debugger.critical(error_msg, "API")
        debugger.api_debug("/chat", 500, f"ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__}")

        error_response = f"ë¯¸ì•ˆí•´! ì§€ê¸ˆ ì¢€ ë³µì¡í•œ ìƒê°ì„ í•˜ëŠë¼ ì œëŒ€ë¡œ ë‹µí•˜ê¸° ì–´ë ¤ì›Œ. ë‹¤ì‹œ ë§í•´ì¤„ë˜? ğŸ˜…\n\n[ë””ë²„ê·¸: {type(e).__name__}]"

        try:
            memory_builder.save_dialogue(user_message, "ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ")
        except:
            debugger.error("ì˜¤ë¥˜ ìƒí™©ì—ì„œ ë©”ëª¨ë¦¬ ì €ì¥ë„ ì‹¤íŒ¨", "MEMORY")

        return {"response": f"[ì‹œìŠ¤í…œ ì˜¤ë¥˜]\n\n{error_response}"}


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """íƒ€ì´í•‘ íš¨ê³¼ê°€ ìˆëŠ” ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… (ë””ë²„ê¹… í¬í•¨)"""
    user_message = request.message
    debugger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ìš”ì²­: {user_message[:50]}...", "STREAMING")

    try:
        # ê°„ë‹¨í•œ ì²˜ë¦¬ (ì˜ë„ ë¶„ë¥˜ ì—†ì´ ë°”ë¡œ ì±„íŒ…)
        context = memory_builder.build_context_from_query(user_message)
        response_text = await chat_with_minicpm_text_only(user_message, context)

        try:
            memory_builder.save_dialogue(user_message, response_text)
            debugger.success("ìŠ¤íŠ¸ë¦¬ë° ëŒ€í™” ë©”ëª¨ë¦¬ ì €ì¥ ì™„ë£Œ", "MEMORY")
        except Exception as mem_error:
            debugger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {mem_error}", "MEMORY")

        debugger.api_debug("/chat/stream", 200, "ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
        # ì´ì œ SSE í˜•ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° (data: JSON\n\n)
        return StreamingResponse(generate_typing_response(response_text), media_type="text/event-stream")

    except Exception as e:
        debugger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì¤‘ ì˜¤ë¥˜: {e}", "STREAMING")
        debugger.api_debug("/chat/stream", 500, f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {type(e).__name__}")

        error_response = "ë¯¸ì•ˆí•´! ì§€ê¸ˆ ë‹µë³€í•˜ê¸° ì–´ë ¤ì›Œì„œ ë‹¤ì‹œ ì‹œë„í•´ì¤˜!"
        return StreamingResponse(generate_typing_response(error_response), media_type="text/event-stream")


@app.post("/chat/image")
async def chat_with_image(request: ImageChatRequest):
    """ì´ë¯¸ì§€ ë¶„ì„ ì±„íŒ… (ë””ë²„ê¹… í¬í•¨)"""
    debugger.info(f"ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­: {request.message[:50]}...", "IMAGE")

    try:
        # ì´ë¯¸ì§€ ë°ì´í„° ê²€ì¦
        try:
            image_bytes = base64.b64decode(request.image_data)
            image = Image.open(io.BytesIO(image_bytes))
            debugger.success(f"ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ: {image.size}", "IMAGE")
        except Exception as img_error:
            debugger.error(f"ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: {img_error}", "IMAGE")
            return {"response": "ì´ë¯¸ì§€ë¥¼ ì½ëŠ”ë° ë¬¸ì œê°€ ìˆì–´. ë‹¤ë¥¸ ì´ë¯¸ì§€ë¡œ ì‹œë„í•´ë³¼ë˜?"}

        # ê°„ë‹¨í•œ ì´ë¯¸ì§€ ë¶„ì„ ì‘ë‹µ (ì‹¤ì œ MiniCPM-V ì´ë¯¸ì§€ ì²˜ë¦¬ëŠ” ë³µì¡í•¨)
        response_text = f"ì´ë¯¸ì§€ë¥¼ í™•ì¸í–ˆì–´ìš”. '{request.message}'ì— ëŒ€í•´ ë‹µí•´ì¤„ê²Œ. ìì„¸í•œ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ë§í•´ì¤˜."

        try:
            memory_builder.save_dialogue(f"{request.message} [ì´ë¯¸ì§€ í¬í•¨]", response_text)
            debugger.success("ì´ë¯¸ì§€ ë¶„ì„ ëŒ€í™” ë©”ëª¨ë¦¬ ì €ì¥ ì™„ë£Œ", "MEMORY")
        except Exception as mem_error:
            debugger.error(f"ì´ë¯¸ì§€ ë¶„ì„ ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {mem_error}", "MEMORY")

        debugger.api_debug("/chat/image", 200, "ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ")
        return {"response": f"[MiniCPM-V ì´ë¯¸ì§€ ë¶„ì„]\n\n{response_text}"}

    except Exception as e:
        debugger.error(f"ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", "IMAGE")
        debugger.api_debug("/chat/image", 500, f"ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {type(e).__name__}")
        return {"response": "ì´ë¯¸ì§€ë¥¼ ë³´ë ¤ê³  í–ˆëŠ”ë° ë­”ê°€ ë¬¸ì œê°€ ìƒê²¼ì–´. ë‹¤ì‹œ ì‹œë„í•´ë³¼ë˜?"}


@app.post("/reset_memory")
async def reset_memory_endpoint():
    """ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ë””ë²„ê¹… í¬í•¨)"""
    debugger.info("ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ìš”ì²­", "MEMORY")

    try:
        reset_memory_handler()
        debugger.success("ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ", "MEMORY")
        debugger.api_debug("/reset_memory", 200, "ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì„±ê³µ")
        return {"message": "ë©”ëª¨ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        debugger.error(f"ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}", "MEMORY")
        debugger.api_debug("/reset_memory", 500, f"ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {type(e).__name__}")
        raise HTTPException(status_code=500, detail=f"ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")


@app.get("/model-status")
async def get_model_status():
    """ëª¨ë¸ ìƒíƒœ í™•ì¸ (ë””ë²„ê¹… í¬í•¨)"""
    debugger.debug("ëª¨ë¸ ìƒíƒœ í™•ì¸ ìš”ì²­", "STATUS")

    # GPU ì •ë³´ ìˆ˜ì§‘
    gpu_info = {}
    if torch.cuda.is_available():
        try:
            gpu_info = {
                "available": True,
                "name": torch.cuda.get_device_name(0),
                "memory_allocated": torch.cuda.memory_allocated(0) / (1024 ** 3),
                "memory_reserved": torch.cuda.memory_reserved(0) / (1024 ** 3),
                "memory_total": torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
            }
        except Exception as gpu_error:
            debugger.error(f"GPU ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {gpu_error}", "GPU")
            gpu_info = {"available": False, "error": str(gpu_error)}
    else:
        gpu_info = {"available": False, "reason": "CUDA not available"}

    status = {
        "minicpm_model": {
            "loaded": minicpm_model is not None,
            "name": "MiniCPM-V-2.6",
            "features": ["text_chat", "image_analysis"] if minicpm_model else [],
            "device": str(minicpm_model.device) if minicpm_model else "N/A",
            "dtype": str(minicpm_model.dtype) if minicpm_model else "N/A"
        },
        "memory_system": {
            "loaded": memory_builder is not None and summarizer is not None
        },
        "gpu_info": gpu_info,
        "debug_info": {
            "debug_enabled": DEBUG_ENABLED,
            "log_to_file": LOG_TO_FILE,
            "logs_directory": "logs" if LOG_TO_FILE else None,
            "config": CONFIG
        },
        "fixes_applied": [
            "ìƒì„¸ ë””ë²„ê¹… ì‹œìŠ¤í…œ ì¶”ê°€",
            "ë³„ë„ í„°ë¯¸ë„ ë¡œê·¸ ì¶œë ¥",
            "ë‹¨ê³„ë³„ ì˜¤ë¥˜ ì¶”ì ",
            "GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§",
            "ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ê°•í™”",
            "íƒ€ì´í•‘ íš¨ê³¼ ê°œì„ ",
            "í˜ë¥´ì†Œë‚˜ ì™¸ë¶€ íŒŒì¼í™” (jia_persona.txt)",
            "ì½”ë“œ ë‚´ í•˜ë“œì½”ë”©ëœ í˜ë¥´ì†Œë‚˜ í…ìŠ¤íŠ¸ ì œê±°"
        ]
    }

    debugger.api_debug("/model-status", 200, "ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì™„ë£Œ")
    return status


@app.get("/debug/logs")
async def get_debug_logs():
    """ìµœê·¼ ë¡œê·¸ ì¡°íšŒ"""
    if not LOG_TO_FILE:
        return {"error": "ë¡œê·¸ íŒŒì¼ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."}

    try:
        logs_dir = "logs"
        if not os.path.exists(logs_dir):
            return {"error": "ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}

        log_files = [f for f in os.listdir(logs_dir) if f.startswith("jia_debug_")]
        if not log_files:
            return {"error": "ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}

        # ìµœì‹  ë¡œê·¸ íŒŒì¼
        latest_log = sorted(log_files)[-1]
        log_path = os.path.join(logs_dir, latest_log)

        with open(log_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            recent_lines = lines[-100:]  # ìµœê·¼ 100ì¤„

        return {
            "log_file": latest_log,
            "total_lines": len(lines),
            "recent_lines": recent_lines
        }
    except Exception as e:
        debugger.error(f"ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", "DEBUG")
        return {"error": str(e)}


@app.get("/debug/system")
async def get_system_debug():
    """ì‹œìŠ¤í…œ ë””ë²„ê·¸ ì •ë³´"""
    from debug_logger import get_system_info

    try:
        system_info = get_system_info()
        monitor_system_resources()
        return system_info
    except Exception as e:
        debugger.error(f"ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}", "DEBUG")
        return {"error": str(e)}


@app.get("/stats")
async def get_stats():
    """ëŒ€í™” í†µê³„ (ë””ë²„ê¹… í¬í•¨)"""
    debugger.debug("ëŒ€í™” í†µê³„ ì¡°íšŒ ìš”ì²­", "STATS")

    try:
        from services.chat_handlers import get_conversation_stats
        stats = get_conversation_stats()
        debugger.success("ëŒ€í™” í†µê³„ ì¡°íšŒ ì™„ë£Œ", "STATS")
        return stats
    except Exception as e:
        debugger.error(f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", "STATS")
        return {"error": str(e)}


if __name__ == "__main__":
    print(" ì±—ë´‡ ì‹œì‘ (ë””ë²„ê·¸ ëª¨ë“œ v6.1)")
    print(f"ğŸ” ë””ë²„ê·¸ í™œì„±í™”: {DEBUG_ENABLED}")
    print(f"ğŸ“„ íŒŒì¼ ë¡œê·¸: {LOG_TO_FILE}")

    if DEBUG_ENABLED:
        print("\n" + "=" * 60)
        print("ğŸ–¥ï¸ ë³„ë„ ë””ë²„ê·¸ í„°ë¯¸ë„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
        print("ëª¨ë“  ì˜¤ë¥˜ì™€ ìƒì„¸ ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("=" * 60 + "\n")

    uvicorn.run("main:app", host="0.0.0.0", port=8001, reload=False)
